{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgoldste\\Anaconda2\\envs\\py35\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update puncuation list in spacy\n",
    "nlp.vocab[\"$\"].is_punct = True\n",
    "nlp.vocab[\"|\"].is_punct = True\n",
    "nlp.vocab[\"+\"].is_punct = True\n",
    "nlp.vocab[\"<\"].is_punct = True\n",
    "nlp.vocab[\">\"].is_punct = True\n",
    "nlp.vocab[\"=\"].is_punct = True\n",
    "nlp.vocab[\"^\"].is_punct = True\n",
    "nlp.vocab[\"`\"].is_punct = True\n",
    "nlp.vocab[\"~\"].is_punct = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_final = pd.read_csv(\"clean_final_news.csv\", encoding = \"utf8\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>published</th>\n",
       "      <th>text</th>\n",
       "      <th>domain_rank</th>\n",
       "      <th>site_url</th>\n",
       "      <th>spam_score</th>\n",
       "      <th>title</th>\n",
       "      <th>response</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAM TANENHAUS</td>\n",
       "      <td>2016-11-20T03:51:00.000+02:00</td>\n",
       "      <td>Privacy Policy Eisenhowers two terms bore this...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Opinion: Donald Trump’s Art of the New Deal?</td>\n",
       "      <td>Not fake</td>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-11-20T00:52:00.000+02:00</td>\n",
       "      <td>Can Trump Save Their Jobs?  by Nelson D. Sc...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Carrier Workers for Trump</td>\n",
       "      <td>Not fake</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kathleen Elkins</td>\n",
       "      <td>2016-11-20T06:18:00.000+02:00</td>\n",
       "      <td>Tuesday, 18 Oct 2016 | 10:25 AM ET CNBC.com Ed...</td>\n",
       "      <td>767.0</td>\n",
       "      <td>cnbc.com</td>\n",
       "      <td>0.008</td>\n",
       "      <td>Tennis star Caroline Wozniacki shares the mone...</td>\n",
       "      <td>Not fake</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JEFF SOMMER</td>\n",
       "      <td>2016-11-20T00:04:00.000+02:00</td>\n",
       "      <td>Continue reading the main story Yet it is poss...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Strategies: It’s Not Just the White House. Cha...</td>\n",
       "      <td>Not fake</td>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>James Rufus Koren</td>\n",
       "      <td>2016-11-20T02:42:00.000+02:00</td>\n",
       "      <td>Wells Fargo hit with new sanctions following f...</td>\n",
       "      <td>609.0</td>\n",
       "      <td>latimes.com</td>\n",
       "      <td>0.264</td>\n",
       "      <td>Wells Fargo hit with new sanctions following f...</td>\n",
       "      <td>Not fake</td>\n",
       "      <td>809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author                      published  \\\n",
       "0      SAM TANENHAUS  2016-11-20T03:51:00.000+02:00   \n",
       "1                NaN  2016-11-20T00:52:00.000+02:00   \n",
       "2    Kathleen Elkins  2016-11-20T06:18:00.000+02:00   \n",
       "3        JEFF SOMMER  2016-11-20T00:04:00.000+02:00   \n",
       "4  James Rufus Koren  2016-11-20T02:42:00.000+02:00   \n",
       "\n",
       "                                                text  domain_rank  \\\n",
       "0  Privacy Policy Eisenhowers two terms bore this...         98.0   \n",
       "1     Can Trump Save Their Jobs?  by Nelson D. Sc...         98.0   \n",
       "2  Tuesday, 18 Oct 2016 | 10:25 AM ET CNBC.com Ed...        767.0   \n",
       "3  Continue reading the main story Yet it is poss...         98.0   \n",
       "4  Wells Fargo hit with new sanctions following f...        609.0   \n",
       "\n",
       "      site_url  spam_score                                              title  \\\n",
       "0  nytimes.com       0.000       Opinion: Donald Trump’s Art of the New Deal?   \n",
       "1  nytimes.com       0.000                          Carrier Workers for Trump   \n",
       "2     cnbc.com       0.008  Tennis star Caroline Wozniacki shares the mone...   \n",
       "3  nytimes.com       0.000  Strategies: It’s Not Just the White House. Cha...   \n",
       "4  latimes.com       0.264  Wells Fargo hit with new sanctions following f...   \n",
       "\n",
       "   response  length  \n",
       "0  Not fake     949  \n",
       "1  Not fake     372  \n",
       "2  Not fake     386  \n",
       "3  Not fake    1005  \n",
       "4  Not fake     809  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert text and response to array \n",
    "x_text = clean_final.text.values\n",
    "y_response = clean_final.response.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove twitter handles from text \n",
    "for idx in range(len(x_text)):\n",
    "    x_text[idx] = re.sub(r'@([A-Za-z0-9_]+)', \"\", str(x_text[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove hyperlinks from text\n",
    "for idx in range(len(x_text)):\n",
    "    x_text[idx] = re.sub(r\"(https|http)\\S+\", \"\", str(x_text[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to eliminate tokens that are pure punctuation, whitespace, or stopword\n",
    "# can be updated based on desired filtering \n",
    "\n",
    "def process_txt(token):\n",
    "    return token.is_punct or token.is_space or token.is_stop or token.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to take array of articles and turn them into nested list of tokens\n",
    "\n",
    "def lemmatize_txt(array):\n",
    "    lemma = []\n",
    "    \n",
    "    for doc in nlp.pipe(array, batch_size=50,\n",
    "                        n_threads=-1):\n",
    "        if doc.is_parsed:\n",
    "            lemma.append([n.lemma_ for n in doc if not process_txt(n)])\n",
    "        \n",
    "        else:\n",
    "            lemma.append(None)\n",
    "    \n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to recombine nested list of tokens into full articles \n",
    "\n",
    "def lemma_combine(lis):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    for i in range(len(lis)):\n",
    "        concat_art = ' '.join(lis[i])\n",
    "        parsed_articles.append(concat_art)\n",
    "    \n",
    "    return parsed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to match \"cleaned\" text back up with response variable\n",
    "\n",
    "def zip_response(observations, response):\n",
    "    response = response.tolist()\n",
    "    \n",
    "    return list(zip(observations, response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create nested list of tokens for each article\n",
    "lem = lemmatize_txt(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create bi-grams for our text \n",
    "phrases = Phrases(lem)\n",
    "bigram = Phraser(phrases)\n",
    "bigram_lem = list(bigram[lem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create tri-grams for our text\n",
    "phrases2 = Phrases(bigram_lem)\n",
    "trigram = Phraser(phrases2)\n",
    "trigram_lem = list(trigram[bigram_lem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recombine full article text for unigrams, bigrams, and trigrams\n",
    "uni_lem_comb = lemma_combine(lem)\n",
    "bi_lem_comb = lemma_combine(bigram_lem)\n",
    "tri_lem_comb = lemma_combine(trigram_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split test and train data using trigram text\n",
    "x_train, x_test, y_train, y_test = train_test_split(tri_lem_comb, y_response, test_size = 0.2, stratify = y_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up pipeline to run TFIDF and Naive Bayes\n",
    "text_pipe = Pipeline([('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82682028315515743"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pipeline to predict on train data and find accuracy\n",
    "predicted_train = text_pipe.predict(x_train)\n",
    "np.mean(predicted_train == y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80687563195146617"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pipeline to predict on test data and find accuracy\n",
    "predicted = text_pipe.predict(x_test)\n",
    "np.mean(predicted == y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Not fake       0.78      1.00      0.88      4715\n",
      "       fake       0.99      0.40      0.57      2208\n",
      "\n",
      "avg / total       0.85      0.81      0.78      6923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get more detailed performance metrics\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predicted, target_names = [\"Not fake\", \"fake\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'tfidf__max_df': (0.75, 0.80, 0.85, 0.90, 0.95, 1.0),\\\n",
    "              'tfidf__min_df': (0.001,0.01,0.1),\\\n",
    "              'clf__alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_pipe, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_clf.best_score_ \n",
    "gs_clf.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
